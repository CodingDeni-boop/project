\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{neurips_2024}
\usepackage{natbib}

\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\title{Escherichia Coli Ceftriaxone Resistance}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  D. Bugatti, N. Dupertuis, A. Giacomuzzi, N. Oberholzer\\
  Foundations of Data Science\\
  Department of Health Science and Technology\\
  ETH Zürich\\
}



\begin{document}
\maketitle

\begin{abstract}
Antibiotic resistance is a growing global health threat, requiring fast and accurate diagnostic tools to guide treatment decisions. In this project, we examine the resistance of Escherichia coli to Ceftriaxone using MALDI-TOF mass spectrometry data from the DRIAMS dataset. The dataset contains 1368 clinical samples with 5999 spectral features. We apply and compare four machine learning models: Support Vector Machine (SVM), Logistic Regression, Random Forest, and k-Nearest Neighbors (kNN). Feature selection and class imbalance handling are used to improve predictive performance. Logistic Regression achieved the highest overall accuracy and ROC AUC, while Random Forest showed perfect recall, illustrating trade-offs between sensitivity and specificity. These results highlight the potential of MALDI-TOF spectral data for predicting antibiotic resistance in E. coli and demonstrate how machine learning can support rapid diagnostics. The study also points to key deployment considerations, such as model robustness and interpretability, that are critical for real-world clinical applications.
\end{abstract}


\section{Introduction}

Antibiotic resistance in bacteria is a critical issue in modern medicine. A study conducted by \citet{IranBacteria} revealed that an astonishing 87.5\% of the bacteria sampled in North Iran were resistant to at least
one antibiotic. It is evident that this percentage has likely increased until now, underscoring the urgency
of addressing this topic. Another study published in 2022 successfully compiled a dataset containing mass
spectrometer results from more than 300’000 samples of bacteria and fungi. This dataset was then used to
train a machine learning model capable of predicting whether a bacteria is resistant to a specific type of
antibiotic; with the ultimate goal of prescribing the most effective treatment to a patient \citep{datasetExplaination}. In this
project, we only selected the samples used to train for the resistance to Ceftriaxone of Escherichia Coli,
with the aim of building and testing different models onto real-world scenarios.
\textbf{\textit{We aim to predict Escherichia coli Ceftriaxone resistance using MALDI-TOF spectra. Our hypothesis is that classical machine learning algorithms can achieve clinically relevant performance (ROC AUC > 0.85).}}

\section{Methods}

\subsection{Data Wrangling and Visualization}
\textbf{\textit{Maybe add Data source? 
}}The dataset consists of an integer label, either 1 or 0, indicating resistance to the antibiotic; mass
spectrometry results, which are floats ranging from 0 to 1, which have been most likely normalized; and a
column named ”Unnamed : 0” which has elements of type string. This column contained a serial number
ending with either “MALDI\_1” or “MALDI\_2”, suggesting it might relate to the machines used for the
observations. To prevent overfitting, we decided to remove this column. Additionally, we checked the
dataset for NA and duplicates but found none. The reformatted dataset now consists in 5999 features
and one label spanning over 1386 samples. 

\subsubsection{Avoiding the Curse of Dimensionality}

As the dataset contains more features than samples, a feature
selection must be performed before fitting with either a filter, wrapper or embedded method. To ensure
that some features can be removed without losing too much information, we created a Pearson’s correlation
matrix of the features and plotted its heatmap with matplotlib \citep{matplotlib} and seaborn \citep{seaborn}.\\

\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{heatmap.png}
	 \vspace{-3em}
	\caption{Heatmap of Pearson's correlation matrix}
\end{figure}

Except the trivial white line that divides the heatmap in half, there are numerous white and dark blue spots, which indicate high positive or negative correlation. Thanks to the graph we can safely assume that most of the variable are correlated, and therefore a filter will not only diminish the probability of overfitting (as mentioned earlier, a higher number of features than samples must always be avoided), but also focus the attention of the model onto non-collinear and highly explanatory features. 

\subsubsection{Assessing Class Imbalance}

Another issue that must be addressed is the class imbalance; 81.5\% of the bacteria samples are resistant to Ceftriaxone, and only 18.5\% are not. There are several ways to help the model focus more onto the minority class. One solution is undersampling the majority class, but we ruled this out, because our dataset already has a relatively low number of samples. Another solution is oversampling the minority, but we rejected it because the duplicated samples would have identical values, therefore increasing the chance of overfitting which is already a concern with this dataset. The solution we chose as best is balancing the class weight in the model. This method doesn't involve dataset alterations, it instead adjusts the training process by giving more importance to mistakes made on the minority class. Specifically, by multiplying such errors by a weight, usually set inversely proportional to the minority class frequency.

\subsubsection{Dataset Split}

The dataset has been split into training and testing sets with an 85\% ratio for training and a 15\% ratio for testing. This leaves 208 samples for testing and 1178 samples for training.

\subsection{Models}
\subsubsection{SVM}
Text here

\subsubsection{Random Forest}
Random Forest (RF) is an ensemble learning algorithm that builds multiple decision trees during training and outputs the class that is the mode of the classes predicted by individual trees. Random Forest was selected for its robustness to noisy and high-dimensional datasets, ease of use, and inherent handling of non-linear relationships without extensive data preprocessing.

To optimize model performance, we performed hyperparameter tuning using grid search with 5-fold cross-validation. The hyperparameters considered were:

\begin{itemize}
    \item \textbf{Number of Trees (\texttt{n\_estimators})}: [300, 350, 400, 450, 500].
    \item \textbf{Maximum Tree Depth (\texttt{max\_depth})}: [None, 10, 20].
    \item \textbf{Minimum Samples for Node Splitting (\texttt{min\_samples\_split})}: [2, 5, 10].
\end{itemize}

To address the significant class imbalance present in the dataset (81.5\% resistant vs. 18.5\% sensitive), the parameter \texttt{class\_weight="balanced"} was set, ensuring increased penalization of misclassification errors for the minority class during training.

Hyperparameter tuning was evaluated using the Receiver Operating Characteristic Area Under the Curve (ROC AUC), specifically selected for its effectiveness in evaluating models trained on imbalanced datasets. This aligns with our aim of achieving clinically relevant predictive performance (ROC AUC > 0.85).

\subsubsection{Logistic Regression}
Text here

\subsubsection{K Nearest Neighbors}
Text here


\subsection{Evaluations Metrics ?}
\textbf{\textit{Clearly justify selected metrics given imbalanced classification scenario:
ROC AUC, Precision, Recall, F1-score, Accuracy.}}

\section{Results}

The following models have been trained and evaluated on the same dataset split, enabling an equal comparison and evaluation of their performance. They have been developed with Scikit-learn's \citep{scikit-learn}, Pandas's \citep{pandas} and numpy's \citep{numpy} algorithms and documentations. 

\subsection{SVM}

Support Vector Machine (SVM) makes his prediction based on hyperplane divisions on the multidimensional feature space. It is therefore extremely important for the success of the model, to select only the highly explanatory features. 

\subsubsection{Model Workflow Explanation}

We created a flowchart which should help visualize the model.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\textwidth]{FlowChart.png}
	 \vspace{0em}
	\caption{Flowchart of the implemented SVM Model}
\end{figure}

As SVM is so dependent on the selected features, we decided to implement a wrapper method which unifies feature selection and SVM model tuning into a grid search cross validation environment to iterate the method for every combination of hyperparameters possible for both feature selection and SVM. At first, a number K of features are selected to train the SVM. The non selected features are discarded. The selection algorithm we deemed best is ANOVA, because it selects the features whose variance contributes the most to the variance in the label. Since SVMs require highly explanatory features, this selection method aligns perfectly with their requirements. After the filtering process, a Support Vector Machine (SVM) model is fitted with a combination of two hyperparameters: C, which represents the strength of the penalty applied through L2 regularization, and Kernel, which determines the shape of the division between classes.

Thanks to pipelining, the same grid search cross validation (5 folds) was used to iterate between all of the combinations of the three hyperparameters:
 
\begin{itemize}
  \item Feature selection's K: [500, 1000, 1250, 1500, 1750]
  \item SVM's C: [10, 100, 125, 150, 175]
  \item SVM's Kernel: ["linear", "poly", "rbf", "sigmoid"]
\end{itemize}
